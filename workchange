import numpy as np

--------------------
def sigmoid(x):
    return 1/(1+np.exp(-x))
def relu(x):
    return np.maximum(0,x)
def softmax(x):
    c = np.max(x)
    exp_a = np.exp(x-c)
    sum_exp_a = np.sum(exp_a)
    y = exp_a / sum_exp_a
    return y
  
  
 -------------------- 
def mean_squared_error(y,t):
    return 0.5 * np.sum((y-t)**2)
## one_hot编码
def cross_entropy_error1(y,t):
    delta = 1e-7
    if y.ndim == 1:
        t = t.reshape(1,-1)
        y = y.reshape(1,-1)
    batch_size = y.shape[0]
    return -np.sum(t * np.log(y + delta)) / batch_size
## 
def cross_entropy_error2(y,t):
    if y.ndim == 1:
        t = t.reshape(1,-1)
        y = y.reshape(1,-1)
    batch_size = y.shape[0]
    return -np.sum(np.log(y[np.arange(batch_size),t] + 1e-7))/batch_size
   
   -----------------------
def numerical_gradient(f,x):
    h = 1e-4
    grad = np.zeros_like(x)
    for idx in range(x.size):
        tmp_val = x[idx]
        x[idx] = tmp_val + h
        fxh1 = f(x)
        
        x[idx] = tmp_val - h
        fxh2 = f(x)
        grad[idx] = (fxh1 - fxh2)/(2*h)
        x[idx] = tmp_val
    return grad
    
    ---------------
class MulLayer:
    def __init__(self):
        self.x = None
        self.y = None
    def forward(self,x,y):
        self.x = x
        self.y = y
        out = x * y
        return out
    def backward(self,dout):
        dx = dout * self.y
        dy = dout * self.x
        return dx,dy

class AddLayer:
    def __init__(self):
        pass
    def forward(self,x,y):
        out = x + y
        return out
    def backward(self,dout):
        dx = dout * 1
        dy = dout * 1
        return dx,dy

class Relu:
    def __init__(self):
        self.x = None
    def forward(self,x):
        self.x = x
        out = relu(x)
        return out
    def backword(self,dout):
        tmp = np.array(self.x>0,dtype=int)
        
        dx = dout.copy()
        dx = dx * tmp
        return dx
    
class Sigmoid:
    def __init__(self):
        self.out = None
    def forward(self,x):
        out = 1 / (1 + np.exp(x))
        self.out = out
        return out
    def backward(self,dout):
        dx = dout * (1.0 - self.out) * self.out
        return dx

class Affine:
    def __init__(self,W,b):
        self.W = W
        self.b = b
        self.x = None
        self.dW = None
        self.dB = None
    def forward(self,x):
        self.x = x
        out = np.dot(x,self.W) + self.b
        return out
    def backward(self,dout):
        dx = np.dot(dout,self.W.T)
        self.dW = np.dot(self.x.T,dout)
        self.db = np.sum(sout,axis=0)
        return dx
    
class SoftmaxWithLoss:
    def __init__(self):
        self.loss = None
        self.y = None
        self.t = None
    def forward(self,x,t):
        self.t = t
        self.y = softmax(x)
        self.loss = cross_entropy_error2(self.y,self.t)
        return self.loss
    def backward(self,dout = 1):
        batch_size = self.t.shape[0]
        dx = (self.y - self.t)/batch_size
        return dx
        
-------------------------------
from collections import OrderedDict
dd.values()

---------

class TwoLayernet2:
    def __init__(self,input_size,hidde_size,output_size,weight_init_std=0.01):
        self.params = OrderedDict()
        self.params['w1'] = weight_init_std * np.random.randn(input_size,hidde_size)
        self.params['b1'] = np.random.randn(hidde_size).reshape(-1,1)
        self.params['w2'] = weight_init_std * np.random.randn(hidde_size,output_size)
        self.params['b2'] = np.random.randn(output_size).reshape(-1,1)
        self.flowchart  = []
        self.flowchart.append(Affine(self.params['w1'],self.params['b1']))
        self.flowchart.append(Relu())
        self.flowchart.append(Affine(self.params['w2'],self.params['b2']))
        self.last = SoftmaxWithLoss()
        
    def predict(self,x):
        out = x
        for f in self.flowchart:
            out = f.forward(out)
        return out
    def accuracy(self,x,t):
        y = self.predict(x)
        y = np.argmax(y,axis = 1)
        t = np.argmax(t,axis=1)
        accuracy = np.sum(y==t)/float(x.shape[0])
        return accuracy
    def gradient(self,x,t):
        out = self.predict(x)
        loss = self.last.forward(x,t)
        dout = self.last.backword()
        layers = list(self.flowchart)
        layers.reverse()
        for f in layers:
            dout = f.backword(dout)
        grads = {}
        grads['W1'] = self.flowchart
