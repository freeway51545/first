def sigmoid(x):
    ar = np.array(x)
    return 1/(1+np.exp(-ar))
class MyNeuron:
    def __init__(self,layers_lst,activation=None,eta=0.01):
        self._w = []
        self._b = []
        for i in range(len(layers_lst)-1):
            self._w.append(np.random.randn(layers_lst[i+1],layers_lst[i]))
            self._b.append(np.random.randn())
        self._eta = eta
        self._activation = lambda x:sigmoid(x)
    #mini-batch:注意x的形状
    def _forpro(self,x_batch):
        ret = x_batch
        for w,b in zip(self._w,self._b):
            ret = self._activation(np.dot(w,ret)+b)
        return ret
    #使用平方误差函数
    def _backpro(self,sigma,oout):
        pass
    def train(self,x_train,y_train):
        train_nums = 10000
        batch_size = 100
        for _ in range(train_nums):
            for i in range(0,len(x_train),batch_size):
                x_batch = x_train[i:i+batch_size]
                y_batch = y_train[i:i+batch_size]
                oret = self._forpro(x_batch)
                loss = np.sum((y - oret))
